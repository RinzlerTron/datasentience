# DataSentience Configuration
# Copy this file to .env and fill in your values
# DO NOT commit .env to version control

# =============================================================================
# REQUIRED CONFIGURATION
# =============================================================================

# NVIDIA NIM API Key (required for AI inference)
# Get your key from: https://build.nvidia.com/explore/discover
NVIDIA_API_KEY=nvapi-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# =============================================================================
# MODEL CONFIGURATION (Whitelist Compliant)
# =============================================================================

# NVIDIA NIM Reasoning Model (must be from whitelist)
# Options: nvidia/llama-3.1-nemotron-nano-8b-v1 (default, fastest)
#          nvidia/llama-3.1-nemotron-70b-instruct
#          nvidia/llama-3.1-nemotron-51b-instruct
REASONING_MODEL=nvidia/llama-3.1-nemotron-nano-8b-v1

# NVIDIA NIM Embedding Model (must be from whitelist)
# Options: nvidia/nv-embedqa-e5-v5 (default, 1024-dim)
#          nvidia/nv-embed-v2 (4096-dim, higher accuracy)
EMBEDDING_MODEL=nvidia/nv-embedqa-e5-v5

# Embedding dimension (auto-set based on model)
# nv-embedqa-e5-v5: 1024
# nv-embed-v2: 4096
EMBEDDING_DIM=1024

# =============================================================================
# PERFORMANCE OPTIMIZATIONS
# =============================================================================

# Redis Cache (OPTIONAL but HIGHLY RECOMMENDED for production)
# Provides 100x cost savings with 90% cache hit rate
# Install Redis: docker run -d -p 6379:6379 redis:7-alpine
# Or use AWS ElastiCache in production
# REDIS_URL=redis://localhost:6379/0

# Cache TTL in seconds (1 hour default)
CACHE_TTL_SECONDS=3600

# Maximum cached embeddings (LRU eviction)
EMBEDDING_CACHE_SIZE=10000

# FAISS Index Configuration
# nlist: Number of clusters for IVF index (sqrt(n_vectors) recommended)
# nprobe: Number of clusters to search (tradeoff: accuracy vs speed)
FAISS_NLIST=100
FAISS_NPROBE=10

# =============================================================================
# AWS CONFIGURATION (Production Deployment)
# =============================================================================

# AWS Secrets Manager (OPTIONAL - use in production instead of env vars)
# Option 1: Use Secret ARN
# SECRET_ARN=arn:aws:secretsmanager:us-east-1:ACCOUNT_ID:secret:datasentience/nvidia-api-key

# Option 2: Use Secret Name
# SECRET_NAME=datasentience/nvidia-api-key

# AWS Region
# AWS_REGION=us-east-1

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Port for FastAPI server (8080 required for SageMaker)
PORT=8080

# Frontend URL for CORS
# Local dev: http://localhost:5173 (Vite)
# Docker: http://localhost
# AWS: https://your-loadbalancer-url.amazonaws.com
FRONTEND_URL=http://localhost:5173

# Request timeout in seconds
REQUEST_TIMEOUT=60

# =============================================================================
# SECURITY & RATE LIMITING
# =============================================================================

# Rate limit per hour per IP
RATE_LIMIT_PER_HOUR=100

# Max tokens per request (prevents abuse)
MAX_TOKENS=1500

# Circuit breaker configuration
CIRCUIT_BREAKER_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT=60

# =============================================================================
# DEMO MODE
# =============================================================================

# Enable demo mode (uses pre-computed responses, no API calls)
# Set to true when NVIDIA_API_KEY is not available
DEMO_MODE=false

# =============================================================================
# LOGGING
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# ENVIRONMENT
# =============================================================================

# Environment: local, staging, production
# Controls validation strictness and logging
ENVIRONMENT=local

# =============================================================================
# DEPLOYMENT NOTES
# =============================================================================

# LOCAL DEVELOPMENT (Quick Start):
# 1. cp .env.example .env
# 2. Add your NVIDIA_API_KEY
# 3. (Optional) Install Redis for caching: docker run -d -p 6379:6379 redis:7-alpine
# 4. (Optional) Uncomment REDIS_URL for 100x performance boost
# 5. Run: uvicorn src.main:app --reload --port 8080

# DOCKER DEPLOYMENT:
# 1. Build: docker build -t datasentience .
# 2. Run: docker run -p 8080:8080 --env-file .env datasentience

# AWS SAGEMAKER DEPLOYMENT:
# 1. Push image to ECR
# 2. Deploy via CloudFormation (deploy.yaml)
# 3. Set SECRET_ARN or SECRET_NAME instead of NVIDIA_API_KEY
# 4. Use AWS ElastiCache Redis for caching
# 5. Endpoint will be: https://runtime.sagemaker.REGION.amazonaws.com/endpoints/NAME/invocations

# PERFORMANCE BENCHMARKS:
# Without Redis: ~2.5s avg query latency, 20 req/min throughput
# With Redis (90% hit rate): ~8.7ms cached, ~1.2s uncached, 600+ req/min throughput
# Cost savings with 90% cache hit rate: 10x ($140K/year â†’ $14K/year at 10M queries)
